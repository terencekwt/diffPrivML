---
title: "Report - diffPrivML"
author: "Terence Tam, Yifan Gong"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Description and problem statement

Machine learning algorithms collectively strive to extract population information and learn general patterns from data sets. The output of machine learning algorithms are generally trained models, where raw data points are distiled to far more compact forms that capture the essence of the origninal data. It is a type of summary aggregation in short that is no different than COUNT and SUM.  


The release of these trained machine learning models, however, comes with a cost--individual data privacy. Oftentimes, machine learning models inadvertently give away information on *individual* data points used in the training data, even though these ML models are meant to release *population* information only.  


To analyze and potentially solve this problem, one unified framework proposed by Cynthia Dwork's paper is the $\epsilon$-differential private mechanism. This is a model that quantifies the degrees of privacy loss for a given data release system, and a tolerance level as a parameter--$\epsilon$. A lot of theoretical research has been done on different ML models with regards to differential privacy, such as these research [here](https://arxiv.org/pdf/1412.7584.pdf), but not enough efforts are being done to apply these in practice.

Therefore, we developed a R package, **diffPrivML**, that aims to put these differential private enhanced ML algorithms in the hands of data practitioners. Just like how a unified framework is developed to measure differential privacy, **diffPrivML** is a precursor in standarizing a library of ML algorithms and workflows that have differential privacy baked in.

## Overview of package

The **diffPrivML** package consists of a library of classifiers implemented as S3 classes, each with differential privacy supported.

* DPNaiveBayesClassifier 
* DPRandomDecisionTreeClassifier
* DPLogisticRegressionClassifier 

For example, to fit a model of **DPNaiveBayesClassifier**, the privacy budget can be specified as part of the input.

```{r, eval=FALSE}
naiveBayesDP <- DPNaiveBayesClassifier(y = trainingSetY, x = trainingSetX, epsilon = 0.05)
```

The model is fitted with the training data input when the classifier object is instantiated.

Each classifier also has its standard ML methods such as **summary()** and **predict()**.

One slight deviation from the original project proposal to also support a neural network based classifier is that it is not straightforward to ship a canned DF enhanced neural network classifier out of the box. Neural networks can be higly customized to first construct the neural network configuration (this is before any training is done), and so the differential privacy parameters have to be customized as well based on the network constructed.

## Motivating Example

It is trivial to capture what kind of privacy data loss a summary query would leak in a database system. For example, a pair of queries like **SELECT SUM(salary)** and **SELECT SUM(salary) WHERE name != ‘Terence Tam’** would do the trick. An adversary just observe the difference between the two results and be able to deduce the individual salary of "Terence Tam".

However, when it comes to the output of a machine learning model, it is not so straightforward as the "summary" returned is rather complex. In addition, understanding in the context of privacy loss the difference between how two training data sets that differs only by one datapoint can lead to two differently trained models is also nontrivial.

To be able to demonstrate this, I will use the **DPRandomDecisionTreeClassifier** in this package as a primer. The advantage of decision tree is that it can be served as a good visualization of the output space created by the model. 

Suppose we have some synthetic data points like so

```{r}
A <- c('0','1')
B <- c('1','0')
class <- c("Y","N")
syntheticData <- as.data.frame(cbind(A, B, class))
head(syntheticData)
```

A and B are predictors and class is the label of the data points. This is an extreme case where the whole dataset only have two data points, and knowing the output of the classifier would definitely leads to privacy loss.

```{r}
library(diffPrivML)
set.seed(777)
rdtDP <- DPRandomDecisionTreeClassifier(Y = syntheticData[,3], X = syntheticData[,1:2], height = 2, numTrees = 1)
rdtDP$trees[[1]]$printTree()
```
```{r}
experiment <- function(){
  rdtDP <- DPRandomDecisionTreeClassifier(Y = syntheticData[,3], X = syntheticData[,1:2], height = 2, numTrees = 1)
  predictions <- predict(rdtDP, syntheticData[,1:2])
  accuracy <- sum(predictions == syntheticData[,3]) / length(syntheticData[,3])
  accuracy
}
hist(replicate(100, experiment()))
```

And now with a stricter differential privacy budget

```{r}
experiment <- function(){
  rdtDP <- DPRandomDecisionTreeClassifier(Y = syntheticData[,3], X = syntheticData[,1:2], height = 2, numTrees = 1, epsilon = 0.005)
  predictions <- predict(rdtDP, syntheticData[,1:2])
  accuracy <- sum(predictions == syntheticData[,3]) / length(syntheticData[,3])
  accuracy
}
hist(replicate(100, experiment()))
```

## Use cases

Now that we have prove the functionality of differential privacy budget, we can dive into actual use cases that can be supported in this package. **diffPrivML** is mainly supported for classification tasks.

### Random Decision Tree Ensemble Classifier

First, let's invoke the caret library, just for the sake that it can cleanly partitiotn training vs test data for us.

```{r}
library(caret)
```

Then, let's use a health dataset, **BreastCancer**, preloaded in the diffPrivML package. I picked health record as that is the quintessential example of data that are sensitive. 

```{r, df_print: paged}
data(BreastCancer)
head(BreastCancer)
```

Running the decision tree classifier

```{r}
Y <- BreastCancer[, 11]
X <- BreastCancer[, 2:10]

trainingIndices <-createDataPartition(Y, p = 0.70, list = FALSE)
trainingSetX <- X[trainingIndices, ]
trainingSetY <- Y[trainingIndices]
testSetX <- X[-trainingIndices, ]
testSetY <- Y[-trainingIndices]

rdtDP <- DPRandomDecisionTreeClassifier(Y = trainingSetY, X = trainingSetX,
                                          height = 4, numTrees = 5)

predictions <- predict(rdtDP, testSetX)
accuracy <- sum(predictions == testSetY) / length(testSetY)
paste("The test set accuracy rate is", accuracy * 100, "%")
```

Again, end user can tweak parameters like depth of trees and number of trees in ensembles to boost the model performance.

### Naive Bayes Classifier

```{r}
data(iris)
y <- iris[, 5]
x <- iris[, 1:4]
trainingIndices <-createDataPartition(y, p = 0.70, list = FALSE)
trainingSetX <- x[trainingIndices, ]
trainingSetY <- y[trainingIndices]
testSetX <- x[-trainingIndices, ]
testSetY <- y[-trainingIndices]
naiveBayesDP <- DPNaiveBayesClassifier(trainingSetY, trainingSetX)
predictions <- predict(naiveBayesDP, testSetX)
accuracy <- sum(predictions == testSetY) / length(testSetY)
accuracy
```

Add the ggplot for accuracy vs epsilon. TODO

```{r}
df <- data.frame(dose=c("D0.5", "D1", "D2"),
                len=c(4.2, 10, 29.5))
ggplot(data=df, aes(x=dose, y=len, group=1)) +
  geom_line() +
  geom_point() +
  scale_size_area()
```

## Contributions

TODO

## Future extensions

This is the basis, can expand to unsupervised learning, and also support multiple methods of perturbation even in one type of classifier.

Also, release the model with hyperparameter tuning.

## References

Zhanglong Ji, Zachary C Lipton, and Charles Elkan. “Differential privacy and machine learning:
  A survey and review”. In: arXiv preprint arXiv:1412.7584 (2014).
  
Jaideep Vaidya, Basit Shafiq, Anirban Basu, and Yuan Hong. "Differentially
  private naive Bayes classification." In Web Intelligence, pages 571–576, 2013.
  
Geetha Jagannathan, Krishnan Pillaipakkamnatt, and Rebecca N. Wright.
  "A practical differentially private random decision tree classifier". In International
  Conference on Data Mining Workshops, pages 114–121, 2009.


