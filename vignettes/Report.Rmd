---
title: "Report - diffPrivML"
author: "Terence Tam, Yifan Gong"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Description and problem statement

Machine learning algorithms collectively strive to extract population information and learn general patterns from data sets. The output of machine learning algorithms are generally trained models, where raw data points are distiled to far more compact forms that capture the essence of the origninal data. It is a type of summary aggregation in short that is no different than COUNT and SUM.  


The release of these trained machine learning models, however, comes with a cost--individual data privacy. Oftentimes, machine learning models inadvertently give away information on *individual* data points used in the training data, even though these ML models are meant to release *population* information only.  


To analyze and potentially solve this problem, one unified framework proposed by Cynthia Dwork's paper is the $\epsilon$-differential private mechanism. This is a model that quantifies the degrees of privacy loss for a given data release system, and a tolerance level as a parameter--$\epsilon$. A lot of theoretical research has been done on different ML models with regards to differential privacy, such as these research [here](https://arxiv.org/pdf/1412.7584.pdf), but not enough efforts are being done to apply these in practice.

Therefore, we developed a R package, `diffPrivML`, that aims to put these differential private enhanced ML algorithms in the hands of data practitioners. Just like how a unified framework is developed to measure differential privacy, `diffPrivML` is a precursor in standardizing a library of ML algorithms and workflows that have differential privacy baked in. End users do not need to worry about *HOW* to perturb the data to release their models; the ML algorithm itself makes the guarantee.

## Overview of package

The `diffPrivML` package consists of a library of classifiers implemented as S3 classes, each with differential privacy supported using algorithms from published research.

* DPNaiveBayesClassifier 
* DPRandomDecisionTreeClassifier
* DPLogisticRegressionClassifier 

For example, to fit a model of `DPNaiveBayesClassifier`, the privacy budget, epsilon, can be specified as part of the input like so.

```{r, eval=FALSE}
naiveBayesDP <- DPNaiveBayesClassifier(y = classLabels, x = predictors, epsilon = 0.05)
```

The model is fitted with the input training data when the classifier object gets instantiated.

Each classifier in `diffPrivML` also has all the standard ML methods such as `summary` and `predict()`. So far, all the ML algorithms in this package uses output perturbation methods for enforcing differential privacy.  

All unit tests are in **/tests** directory.

One slight deviation from the original project proposal to also support neural network based classifier is that it is not straightforward to ship a default DF enhanced neural network classifier out of the box. Neural networks can be higly customized to first construct the neural network configuration (this is even before any training is done), and so the differential privacy parameters have to be customized as well based on the network constructed.

## Use cases

First, let's invoke our library! Also will call the caret library, just for the sake that it can cleanly partition training vs test data for us.

```{r, results='hide', message=FALSE}
library(diffPrivML)
library(caret)
set.seed(50) # for reproducibility
```

### Random Decision Tree Ensemble Classifier

Random decision tree classifier uses the concept of ensemble, where we construct a series of base individual classifiers (trees) and then combine these trees together to form a forest (ensemble). Random decision tree, unlike traditional classification decision trees, are constructed by partitioning the data randomly at each node--this is a rather important detail.

The advantage for such "random" trees allow the math of calculating sensitivity of differential privacy bounds to be much easier. This approach allows us to first compute entire decision tree and then simply adds noise at the end to the leave nodes. On the other hand, with more complex traditional decision tree that uses heuristics like Gini index and information criterion, the differential privacy sensitivity bound is not so straightforward. This is the reason why we implemented this from scratch as most popular decision tree packages have these heuristics baked in.

Let's demonstrate this random decision tree classifier, using the categorical health dataset `BreastCancer` preloaded in the diffPrivML package. We picked health records as that is the quintessential data that needs privatization!

```{r}
data(BreastCancer)
head(BreastCancer)
```

First let's actually split the BreastCancer data into training set and test set, and fit the classifier model with the training data, calling our `DPRandomDecisionTreeClassifier` api.

```{r}
Y <- BreastCancer[, 11]
X <- BreastCancer[, 2:10]

trainingIndices <- createDataPartition(Y, p = 0.70, list = FALSE)
trainingSetX <- X[trainingIndices, ]
trainingSetY <- Y[trainingIndices]
testSetX <- X[-trainingIndices, ]
testSetY <- Y[-trainingIndices]

rdtDP <- DPRandomDecisionTreeClassifier(Y = trainingSetY, X = trainingSetX,
                                        height = 4, numTrees = 6, epsilon = 10.0)
summary(rdtDP)
```

The `summary` method will give all the parameters associated with the trained model. Again, end user can tweak parameters like depth of trees (height) and number of trees in the ensemble (numTrees) to boost the model performance.

For predicting the test set, we then run `predict` on the trained model.

```{r}
predictions <- predict(rdtDP, testSetX)
accuracy <- sum(predictions == testSetY) / length(testSetY)
paste("The test set accuracy rate is", accuracy * 100, "%")
```

Note this classifier is only supported for categorical data. `DPNaiveBayesClassifier`, on the other hand, support mixed categorical / numerical attributes.

Now, one thing end users of this package have to keep in mind is the tradeoff between how much to reveal in their trained model (privacy) vs how effective the classification is (accuracy). Below, we graphed the test set accuracy of the above dataset with various epsilon (privacy budget) to show this decay. A tighter privacy bound (as epsilon decreases), the accuracy of the classifier gradually decreases.

```{r}
eps <- c(3, 1, 0.7, 0.4, 0.01, 0.0002)

accuracy <- sapply(eps, function(epsilon) {
  rdtDP <- DPRandomDecisionTreeClassifier(Y = trainingSetY, X = trainingSetX, height = 3, 
                                          numTrees = 6, epsilon = epsilon)
  predictions <- predict(rdtDP, testSetX)
  sum(predictions == testSetY) * 100 / length(testSetY)
})

df <- data.frame(epsilon = eps,
                accuracy = accuracy)
ggplot(data=df, aes(x=epsilon, y=accuracy, group=1)) +
  geom_line() +
  geom_point() +
  scale_size_area()
```

This implementation is inspired by the research done [here](http://paul.rutgers.edu/~geetha/rdt.pdf).

### Naive Bayes Classifier

Another classifier we implemented is the differential private naive bayes classifier. This classifier support mixed numerical and categorical predictors, as well as multivariate labels. For end user, this is a great general all-purpose privacy preserving classifier readily available out of the toolbox.

The elegance of naive bayes classifier, albeit the notion of being simple, is that it doesn't need to sequentially feed in batches of training data to minimize a loss function. The whole dataset is read once and all the prior probabilities and conditional probabilites are computed. That make this classifier an attractive option for calulating differential privacy sensitivity bounds.

To invoke `DPNaiveBayesClassifier`, we simply the same set of steps as before. First get your data ready in the right format.

```{r}
data(iris)
y <- iris[, 5]
x <- iris[, 1:4]
trainingIndices <- createDataPartition(y, p = 0.70, list = FALSE)
trainingSetX <- x[trainingIndices, ]
trainingSetY <- y[trainingIndices]
testSetX <- x[-trainingIndices, ]
testSetY <- y[-trainingIndices]
```

Then invoke the `DPNaiveBayesClassifier`, and also get the summary of the model fitted.

```{r}
naiveBayesDP <- DPNaiveBayesClassifier(trainingSetY, trainingSetX, epsilon = 0.7)
summary(naiveBayesDP)
```
Then try to predict the test data set like so.

```{r}
predictions <- predict(naiveBayesDP, testSetX)
accuracy <- sum(predictions == testSetY) / length(testSetY)
paste("The test set accuracy rate is", accuracy * 100, "%")
```

Similar performance is shown when we tighten up epsilon.

```{r}
eps <- c(1, 0.7, 0.4, 0.1, 0.05)

accuracy <- sapply(eps, function(epsilon) {
  naiveBayesDP <- DPNaiveBayesClassifier(trainingSetY, trainingSetX, epsilon = epsilon)
  predictions <- predict(naiveBayesDP, testSetX)
  sum(predictions == testSetY) * 100 / length(testSetY)
})

df <- data.frame(epsilon = eps,
                accuracy = accuracy)
ggplot(data=df, aes(x=epsilon, y=accuracy, group=1)) +
  geom_line() +
  geom_point() +
  scale_size_area()
```

This implementation is inspired by research done [here](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690067).

### Logistic Regression Classifier

Lastly, we implemented differential private logistic regression. Logistic regression is a type of model for binary classification problems. Karalikta et al. (2011) described an output pertubation mechanism that's applicable to regularized logistic regression classifiers. This package implements that mechanism via first calling package `glmnet` to get the weights that minimize the loss, and then adding noise subject to the privacy budget and regularization parameter. 

We would also like to credit our noise function to Staal A. Vinterbo and his R package `PrivateLR`. Unlike `PrivateLR` which built the logistic regression model from scratch, we use `glmnet` for the weights and enable two regularization techniques: ridge and lasso.

Just as before, the classifier can be invoked with a similar fashion.
```{r}
data(iris)
y <- factor(iris$Species == "setosa")
x <- as.matrix(iris[, 1:4])
trainingIndices <- createDataPartition(y, p = 0.70, list = FALSE)
trainingSetX <- x[trainingIndices, ]
trainingSetY <- y[trainingIndices]
testSetX <- x[-trainingIndices, ]
testSetY <- y[-trainingIndices]

lrDP <- DPLogisticRegressionClassifier(trainingSetY, trainingSetX, lambda = 1, epsilon = 2)
summary(lrDP)
```

Then try to predict the test data set and specify the prediction outcome to be either `probabilities` or `classes`.

```{r}
predictions <- predict(lrDP, testSetX, "classes")
accuracy <- sum(predictions == testSetY) / length(testSetY)
paste("The test set accuracy rate is", accuracy * 100, "%")
```

## Synthetic Data - Proof of differential privacy

It is trivial to capture what kind of privacy data loss a summary query would leak in a database system. For example, a pair of queries like `SELECT SUM(salary)` and `SELECT SUM(salary) WHERE name != ‘Terence Tam’` would do the trick. An adversary just observe the difference between the two results and be able to deduce the individual salary of "Terence Tam".

However, when it comes to the output of a machine learning model, it is not so straightforward as the "summary" returned is rather complex. In addition, understanding in the context of privacy loss the difference between how two training data sets that differs only by one datapoint can lead to two differently trained models is also nontrivial.

To be able to demonstrate this, I will use the `DPRandomDecisionTreeClassifier` again in this package. The advantage of decision tree is that it can be served as a good visualization of the output space created by the model. 

Suppose we have some synthetic data set that looks like this

```{r}
A <- c('0','1','1')
B <- c('1','0','0')
class <- c("Y","N","N")
syntheticData <- as.data.frame(cbind(A, B, class))
head(syntheticData)
```

A and B are predictors and class is the label for the data points. This is an extreme case where the whole training dataset only have three data points. Notice how the data points are very orthogonal and mutually exclusive, where class == 'Y' only when B == '1'. Therefore, in this toy example, knowing exactly the output of the classifier would definitely give inference on what data points are used in the training set, leading to privacy loss.

Suppose we run a non-private random decision tree classifier with the three data points as input like so.

```{r}
rdtDP <- DPRandomDecisionTreeClassifier(Y = syntheticData[,3], X = syntheticData[,1:2], height = 2, numTrees = 1)
rdtDP$trees[[1]]$printTree()
```

In this instance, the classifier decide to first partition on A, then B, hence A is the root and B are the children nodes. There are two branches because A has two possible values (0 and 1).

When epsilon is not specified in our DP classifier, that will default to non-private algorithm (in a sense, that means epsilon is infinite). Also here, explicitly setting numTree = 1 so we only have 1 tree in our classifer for sake of discussion.

Now suppose we train the classifer 100 times, it is with certainty that for the data point (A=0,B=1,class=Y), the predicted output is the same, which is 'Y'.

```{r}
experiment <- function(){
  rdtDP <- DPRandomDecisionTreeClassifier(Y = syntheticData[,3], X = syntheticData[,1:2], 
                                          height = 2, numTrees = 1)
  predict(rdtDP, syntheticData[1,1:2]) # predict what class when (A=0,B=1)
}
table(replicate(100, experiment()))
```

What if then we exclude this data point (A=0,B=1,class=Y) from the original training data? Not suprisingly, for non-private decision tree, the generated tree is then just A -> B, as one of the branch get pruned out as none of the training data (the remaining 2 data points) traverses the A = 0 path.

Hence this is what makes the original decision tree non-private! Two training datasets that are just 1 data different renders two distinguishable model outputs, in the form of two different trees.

To remediate this data privacy loss, now let's suppose we are tightening up the privacy budget (setting epsilon to something low like 0.005), and then try compare the two models (one with 2 training data and one with 3 training data points).

Training with data point (A=0,B=1,class=Y) included.

```{r}
rdtDP1 <- DPRandomDecisionTreeClassifier(Y = syntheticData[,3], X = syntheticData[,1:2], height = 2, numTrees = 1, epsilon = 0.005)
```

Training without data point (A=0,B=1,class=Y) included.

```{r}
rdtDP2 <- DPRandomDecisionTreeClassifier(Y = syntheticData[-1,3], X = syntheticData[-1,1:2], height = 2, numTrees = 1, epsilon = 0.005)
```

```{r}
experimentWithPrivacy <- function(Y, X) {
  rdtDP <- DPRandomDecisionTreeClassifier(Y = Y, X = X, height = 2, numTrees = 1, epsilon = 0.005)
  predict(rdtDP, syntheticData[1,1:2]) # predict what class when (A=0,B=1)
}
#with data point included
table(replicate(100, experimentWithPrivacy(Y = syntheticData[,3], X = syntheticData[,1:2])))
#without data point included
table(replicate(100, experimentWithPrivacy(Y = syntheticData[-1,3], X = syntheticData[-1,1:2])))
```

As you can see, with stricter differential privacy enforced, the two models have enough noise added and become more "alike" such that the adversary can't guess which dataset is actually used given the generated model ouput (the trees).

The generated decision trees "looks" the same as expected. Even though rdtDP2 was missing the extra training data point, our DP decision tree algorithm leave that branch unpruned and generate noise to make the output less distinguishable. 
```{r}
rdtDP1$trees[[1]]$printTree()
rdtDP2$trees[[1]]$printTree()
```

(The order of partitioning is random, so it could be A as root or B as root.)

Hence this proves the basis of how differential privacy is enforced.

## Contributions

We spent equal of time researching in the beginning to understand the concept of differential privacy and its implementations. Differential privacy is very theoretical and academic in nature and requires some amount of learning curve. Terence is responsible for implementing the decision tree and naive bayes algorithm, and Yifan is responsible for the logistic regression algorithm. 

## Future extensions

So far, our package deals specifically with privatizing *training* data in the machine learning workflow. Oftentimes, machine learning algorithms have yet another set of data (the hold out set or validation set) for hyperparameter tuning and cross validations. These data also influence the final model output and so requires another set of privatization procedure to enforce differential privacy.

Again, `diffPrivML` serves as a precursor in standardizing a library of ML algorithms and workflows that have differential privacy baked in. Classification and output perturbations are just starting points. The package can naturally be extended to unsupervised learning as well as other perturbation methods (input and objective perturbations).

Thank you!

## References

Geetha Jagannathan, Krishnan Pillaipakkamnatt, and Rebecca N. Wright.
  "A practical differentially private random decision tree classifier". In International
  Conference on Data Mining Workshops, pages 114–121, 2009.

Kamalika Chaudhuri, Claire Monteleoni, Anand D. Sarwate. "Differentially Private Empirical Risk Minimization".
  In Journal of Machine Learning Research 12 (2001) 1069-1109.

Jaideep Vaidya, Basit Shafiq, Anirban Basu, and Yuan Hong. "Differentially
  private naive Bayes classification." In Web Intelligence, pages 571–576, 2013.
  
Zhanglong Ji, Zachary C Lipton, and Charles Elkan. “Differential privacy and machine learning:
  A survey and review”. In: arXiv preprint arXiv:1412.7584 (2014).

